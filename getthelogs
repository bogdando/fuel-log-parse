#!/bin/bash
# Source http://git.openstack.org/cgit/openstack-infra/tripleo-ci/tree/scripts/getthelogs
# but the source is no longer maintainer or cared about
set -eu -o pipefail

function usage(){
  echo "Helper script for downloading tripleo-ci jobs logs"
  echo
  echo "Example:"
  echo "getthelogs http://logs.openstack.org/00/123456/7/check/gate-tripleo-ci-foo/d3adbeef"
  echo
  echo "Downloads the logs and starts a shell from the logs root directory"
}

function finish(){
  rc=${rc:-$?}
  echo Processed URLs:
  cat $urls
  rm -f $urls
  trap - EXIT
  cd $TDIR/../
  echo "Download job exited ${rc}"
  PS1="JOBLOGS ]\$  " bash --noprofile --norc
}

function get_jobs(){
  local drop="\b(ara|ara_oooq|docs|build|stackviz)\b"
  local entries=$(curl -sLk "$1" 2> /dev/null | $filter | grep -E '\[DIR|href=\S+\/?<' | grep -vE "${drop}" | sed -e "s,.*href=[\"\']\([^\"\']*\)[\"\'].*,${1}\1,g")
  [ "$entries:-" ] || return
  IFS=$'\n'
  set +u
  for d in $entries; do
    echo "Processing URLs for $d"
    echo $d | grep -Eq '\.\.\/|\/\/$' && continue
    if echo $d | grep -q /$; then  # list directories via recursion
      cat $urls | grep -q "${d%*/}/ " || get_jobs "${d%*/}/"
    elif [ "$filter" != "cat" ]; then  # if compressed, also list files
      cat $urls | grep -q "$d " || echo "$d" >> $urls
    fi
  done
  set -u
}

[[ "${1:--}" =~ ^\s+?- ]] && (usage; exit 1)
type -p wget 2>&1 >/dev/null || ( echo "Please install a wget tool!"; exit 127 )
trap finish EXIT SIGINT SIGTERM
urls=$(mktemp /tmp/tmp.XXXXXXXXXX)

WORKERS=6
BASEURL=${1%/}
SC=$(dirname $BASEURL | grep -o \/ | wc -w)
if [[ $BASEURL =~ 'logs.rdoproject' && SC -le 9 ]] ||\
   [[ $BASEURL =~ 'logs.rdoproject.org/openstack-periodic' && SC -le 5 ]]; then
  BASEURL=${BASEURL}/logs
elif [[ ! $(basename $BASEURL) == 'logs' && SC -le 7 ]]; then
  BASEURL=${BASEURL}/logs
fi
console="$BASEURL/job-output.txt.gz"
console_old="$BASEURL/console.txt.gz"
TDIR=${BASEURL##*http://}
TDIR=${TDIR##*https://}
TDIR=/tmp/${TDIR}
mkdir -p $TDIR
cd /tmp

echo "Target dir for download: $TDIR"
echo Will download logs from the following URLs:
filter="cat"
curl -sIk "$BASEURL/" | grep -qi 'content-encoding: gzip' && filter="zcat"
echo $console >> $urls
echo $console_old >> $urls
get_jobs "$BASEURL/"
rm -f wget-jobs.txt
while read -r d; do
  args="\"-nv -nc --no-use-server-timestamps --no-check-certificate \
    --accept-regex='(\.co?nf|\.ya?ml|\.json|\.log|\.txt)(\.gz)?$|messages$' \
  --reject='index.html*' \
  --recursive -l 10 --domains logs.openstack.org,logs.rdoproject.org,storage.gra.cloud.ovh.net \
  --no-parent \
  -erobots=off --wait 0.1 ${d}\""
  echo "$args" >> wget-jobs.txt
done < <(cat -- $urls)

cat wget-jobs.txt | sed -n '{p;p}' | shuf > wget-jobs-shuf.txt
# Do not fail if something is missing / cannot be downloaded
set +e
cat wget-jobs-shuf.txt  | xargs -r -n1 -P ${WORKERS} -I{} sh -c "wget --header='Accept-Encoding: gzip' {} | $filter"
cd "$TDIR"
if [ "$filter" = "cat" ]; then
  find . -type f -name "*.gz" | xargs -r -n1 gunzip
else
  find . -type f -name "*.*" | xargs -r -n1 -I{} bash -c 'mv -f "{}" "{}_" && zcat "{}_" > "{}" && rm -f "{}_"'
fi
