#!/bin/bash -xxx
# Source http://git.openstack.org/cgit/openstack-infra/tripleo-ci/tree/scripts/getthelogs
# but the source is no longer maintainer or cared about
RETRY_LOCAL=${RETRY_LOCAL:-false}
set -eu -o pipefail
ca=${2:-}
cert=${3:-}

[ -n "$ca" ] && ca="--ca-directory=$ca"
[ -n "$cert" ] && cert="--certificate=$cert"

function usage(){
  echo "Helper script for downloading tripleo-ci jobs logs"
  echo
  echo "Example:"
  echo "getthelogs http://logs.openstack.org/00/123456/7/check/gate-tripleo-ci-foo/d3adbeef ca-dir certfile.pem"
  echo "RETRY_LOCAL=/tmp/tmp.aSNBKIWKHO getthelogs http://logs.openstack.org/00/123456/7/check/gate-tripleo-ci-foo/d3adbeef"
  echo
  echo "Downloads the logs and starts a shell from the logs root directory"
}

function finish(){
  rc=${rc:-$?}
  echo "Processed URLs stored in $urls":
  cat $urls
  #if [ "$RETRY_LOCAL" != "false" ]; then
    #rm -f $urls
    #rm -f /tmp/wget-jobs-shuf.txt /tmp/wget-jobs.txt
    #rm -fr "$fentries"
  #fi
  trap - EXIT
  cd $TDIR/../
  echo "Download job exited ${rc}"
  PS1="JOBLOGS ]\$  " bash --noprofile --norc
}

function get_jobs(){
  local drop="\b(ara|ara_oooq|docs|build|stackviz)\b"
  local lurls=$(mktemp "$fentries/urls.XXXXXXXXXX")
  local entries=$(mktemp "$fentries/entries.XXXXXXXXXX")
  curl -sLk "$1" 2> /dev/null | $filter |\
    grep -E '\[DIR|href=\S+\/?<' | grep -vE "${drop}" |\
    sed -e "s,.*href=[\"\']\([^\"\']*\)[\"\'].*,${1}\1,g" |\
    grep -v '\?' > $entries
  [ $(cat $entries | wc -l) -eq 0 ] && return
  set +u
  while read -r d; do
    echo "Processing URLs for $d"
    echo $d | grep -Eq '\.\.\/|\/\/$' && continue
    if echo $d | grep -q /$; then  # list directories via recursion
      cat $lurls | grep -q "${d%*/}/ " || get_jobs "${d%*/}/"
    else  # also list files
      grep -q "$d" $urls && continue
      cat $lurls | grep -q "$d " || echo "$d" >> $urls
    fi
  done < <(cat -- $entries)
  set -u
}

[[ "${1:--}" =~ ^\s+?- ]] && (usage; exit 1)
type -p wget 2>&1 >/dev/null || ( echo "Please install a wget tool!"; exit 127 )
trap finish EXIT SIGINT SIGTERM
if [ "$RETRY_LOCAL" = "false" ]; then
  urls=$(mktemp -t tmp.XXXXXXXXXX)
  fentries=$(mktemp -d -t entries-XXXXXXXXXX)
else
  fentries=$(ls -tr1d /tmp/entries-* | tail -1)
  urls=$RETRY_LOCAL
fi

WORKERS=6
BASEURL=${1%/}
SC=$(dirname $BASEURL | grep -o \/ | wc -w)
if [ $SC -le 9 -o $SC -le 5 ]; then
  BASEURL=${BASEURL}/logs
elif [ $(basename $BASEURL) != 'logs' -a $SC -le 7 ]; then
  BASEURL=${BASEURL}/logs
fi
TDIR=${BASEURL##*http://}
TDIR=${TDIR##*https://}
TDIR=/tmp/${TDIR}
mkdir -p $TDIR
cd /tmp

echo "Target dir for download: $TDIR"
echo Will download logs from the following URLs:
filter="cat"
curl -sIk "$BASEURL/" | grep -qi 'content-encoding: gzip' && filter="zcat"
[ "$RETRY_LOCAL" = "false" ] &&  get_jobs "$BASEURL/"
rm -f wget-jobs.txt
while read -r d; do
  args="\"-nv -nc --no-use-server-timestamps --no-check-certificate \
    --accept-regex='(\.co?nf|\.ya?ml|\.json|\.log|\.txt)(\.gz)?$|messages$' \
  --reject='index.html*' \
  --recursive -l 10 \
  --no-parent \
  -erobots=off --wait 0.1 ${d}\""
  echo "$args" >> wget-jobs.txt
done < <(cat -- $urls)

cat wget-jobs.txt | shuf > wget-jobs-shuf.txt
# Do not fail if something is missing / cannot be downloaded
set +e
cat wget-jobs-shuf.txt  | xargs -r -n1 -P ${WORKERS} -I{} sh -c "wget $ca $cert --header='Accept-Encoding: gzip' {} | $filter"
set -e
cd "$TDIR"
if [ "$filter" = "cat" ]; then
  find . -type f -name "*.gz" | xargs -r -n1 gunzip
else
  find . -type f -name "*.*" | xargs -r -n1 -I{} bash -c 'mv -f "{}" "{}_" && zcat "{}_" > "{}" && rm -f "{}_"'
fi
